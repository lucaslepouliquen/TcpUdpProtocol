# TCP/IP pour l'Ing√©nieur Infrastructure

> Guide th√©orique et pratique pour DevOps/Kubernetes

[![Made for DevOps](https://img.shields.io/badge/Made%20for-DevOps-blue)](https://github.com)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-326CE5?logo=kubernetes)](https://kubernetes.io)
[![Azure](https://img.shields.io/badge/Azure-AKS-0078D4?logo=microsoft-azure)](https://azure.microsoft.com)

---

## Table des mati√®res

- [Introduction](#-introduction)
- [PARTIE 1 - Fondamentaux Th√©oriques](#partie-1---fondamentaux-th√©oriques)
  - [1. Architecture en Couches](#1-architecture-en-couches-mod√®le-ositcp-ip)
  - [2. Adressage IP et Subnetting](#2-adressage-ip-et-subnetting)
  - [3. TCP vs UDP](#3-tcp-vs-udp--protocoles-de-transport)
  - [4. Routage IP](#4-routage-ip-et-tables-de-routage)
  - [5. MTU et Fragmentation](#5-mtu-maximum-transmission-unit-et-fragmentation)
  - [6. NAT](#6-nat-network-address-translation)
  - [7. ARP](#7-arp-address-resolution-protocol)
  - [8. DNS](#8-dns-domain-name-system)
  - [9. Timeouts et Keep-Alive](#9-timeouts-keep-alive-et-connection-pooling)
- [PARTIE 2 - Applications Pratiques](#partie-2---applications-pratiques-contexte-devopskubernetes)
  - [10. TCP/IP dans Kubernetes](#10-tcpip-dans-kubernetes--architecture-r√©seau)
  - [11. Services Kubernetes](#11-services-kubernetes-et-kube-proxy)
  - [12. Application Gateway et Ingress](#12-azure-application-gateway-et-ingress)
  - [13. Network Policies](#13-network-policies--firewall-l3l4-dans-kubernetes)
  - [14. Troubleshooting](#14-troubleshooting-r√©seau--outils-et-m√©thodologie)
  - [15. Cas Pratiques](#15-cas-pratiques--contexte-promodaks)
  - [16. Best Practices](#16-best-practices-tcpip-pour-infrastructure-kubernetes)
- [Ressources](#-ressources-recommand√©es)

---

## Introduction

Ce guide couvre les aspects fondamentaux du protocole TCP/IP pour un ing√©nieur infrastructure, avec des exemples concrets d'application dans un contexte DevOps et Kubernetes.

**Pourquoi ce guide ?**
- Comprendre les fondamentaux th√©oriques
- Appliquer les concepts √† Kubernetes/Azure
- D√©bugger efficacement les probl√®mes r√©seau
- Optimiser les performances r√©seau en production

---

# PARTIE 1 - Fondamentaux Th√©oriques

## 1. Architecture en Couches (Mod√®le OSI/TCP-IP)

Le mod√®le TCP/IP est organis√© en **4 couches principales**, chacune ayant des responsabilit√©s sp√©cifiques :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Couche 4 - APPLICATION             ‚îÇ  HTTP, DNS, SSH, FTP
‚îÇ  (Interface utilisateur)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Couche 3 - TRANSPORT               ‚îÇ  TCP (fiable) / UDP (rapide)
‚îÇ  (Segmentation, contr√¥le de flux)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Couche 2 - INTERNET (IP)           ‚îÇ  IPv4, IPv6, ICMP, ARP
‚îÇ  (Routage entre r√©seaux)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Couche 1 - LIAISON/PHYSIQUE        ‚îÇ  Ethernet, WiFi, MAC
‚îÇ  (Transmission physique)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Encapsulation

Chaque couche encapsule les donn√©es de la couche sup√©rieure en ajoutant son propre en-t√™te :

```
Application ‚Üí Segment (TCP/UDP) ‚Üí Paquet (IP) ‚Üí Trame (Ethernet)
```

---

## 2. Adressage IP et Subnetting

### 2.1 IPv4

- **Format** : 32 bits (4 octets), notation d√©cimale point√©e
- **Exemple** : `192.168.1.10`
- **Classes historiques** : A (/8), B (/16), C (/24)
- **Aujourd'hui** : CIDR (Classless Inter-Domain Routing)

**Adresses priv√©es (RFC 1918)** :
```
10.0.0.0/8        ‚Üí 10.0.0.0 - 10.255.255.255
172.16.0.0/12     ‚Üí 172.16.0.0 - 172.31.255.255
192.168.0.0/16    ‚Üí 192.168.0.0 - 192.168.255.255
```

### 2.2 Notation CIDR et Calcul

**Formule rapide** : `2^(32-masque) - 2` (network + broadcast)

| CIDR | Masque | IPs totales | IPs utilisables |
|------|--------|-------------|-----------------|
| /24  | 255.255.255.0 | 256 | 254 |
| /20  | 255.255.240.0 | 4,096 | 4,094 |
| /16  | 255.255.0.0 | 65,536 | 65,534 |
| /12  | 255.240.0.0 | 1,048,576 | 1,048,574 |

### Exemple Subnetting

Diviser `10.0.0.0/16` en sous-r√©seaux `/24` :

```
10.0.0.0/24   ‚Üí 10.0.0.1 - 10.0.0.254
10.0.1.0/24   ‚Üí 10.0.1.1 - 10.0.1.254
10.0.2.0/24   ‚Üí 10.0.2.1 - 10.0.2.254
...
10.0.255.0/24 ‚Üí 10.0.255.1 - 10.0.255.254
```

---

## 3. TCP vs UDP : Protocoles de Transport

### 3.1 TCP (Transmission Control Protocol)

**Caract√©ristiques** :
- Orient√© connexion (positif)
- Fiable (garantit livraison et ordre) (positif)
- Contr√¥le de flux et de congestion (positif)
- Overhead plus √©lev√© (n√©gatif)

#### Three-Way Handshake

```
Client                    Server
  |                         |
  |-------- SYN -------->   |  (1) Demande connexion
  |                         |
  |<----- SYN-ACK -------   |  (2) Acceptation
  |                         |
  |-------- ACK -------->   |  (3) Confirmation
  |                         |
  |=== ESTABLISHED ========>|  Connexion √©tablie
```

#### √âtats TCP Importants

| √âtat | Description |
|------|-------------|
| `LISTEN` | En attente de connexion entrante |
| `SYN_SENT` | SYN envoy√©, attente SYN-ACK |
| `ESTABLISHED` | Connexion active |
| `TIME_WAIT` | Connexion ferm√©e, attente s√©curit√© |
| `CLOSE_WAIT` | En attente de fermeture locale |
| `FIN_WAIT` | FIN envoy√©, attente confirmation |

#### Flags TCP

| Flag | Signification |
|------|---------------|
| `SYN` | Synchronisation (√©tablir connexion) |
| `ACK` | Acknowledgement (confirmer r√©ception) |
| `FIN` | Finish (terminer connexion) |
| `RST` | Reset (fermeture abrupte) |
| `PSH` | Push (envoyer imm√©diatement) |
| `URG` | Urgent (donn√©es prioritaires) |

#### Retransmission TCP

- TCP retransmet automatiquement les paquets perdus
- **RTO** (Retransmission TimeOut) : ajust√© dynamiquement
- **Fen√™tre TCP** : contr√¥le de flux via fen√™tre glissante

### 3.2 UDP (User Datagram Protocol)

**Caract√©ristiques** :
- Sans connexion (positif)
- Latence minimale (positif)
- Overhead minimal (positif)
- Pas de garantie de livraison (n√©gatif)
- Pas de contr√¥le de flux (n√©gatif)

**Use cases** :
- üéÆ Gaming (latence critique)
- üìû VoIP (temps r√©el)
- üì∫ Streaming vid√©o
- üîç DNS (requ√™tes courtes)
- üìä M√©triques (tol√©rance perte)

---

## 4. Routage IP et Tables de Routage

### Table de Routage

Dictionnaire indiquant o√π envoyer les paquets selon leur IP de destination.

**Structure d'une entr√©e** :
```
Destination    Gateway         Netmask         Interface
10.0.1.0       0.0.0.0         255.255.255.0   eth0
192.168.1.0    0.0.0.0         255.255.255.0   eth1
0.0.0.0        10.0.1.1        0.0.0.0         eth0  ‚Üê Route par d√©faut
```

### Commandes Utiles

```bash
# Voir la table de routage
ip route show
route -n

# V√©rifier quelle route est utilis√©e
ip route get 8.8.8.8

# Ajouter une route statique
ip route add 192.168.2.0/24 via 10.0.1.1
```

### Protocoles de Routage

| Protocole | Type | Usage |
|-----------|------|-------|
| **RIP** | Distance Vector | Petits r√©seaux, max 15 hops |
| **OSPF** | Link State | R√©seaux entreprise, scalable |
| **BGP** | Path Vector | Internet, routage inter-AS |

---

## 5. MTU (Maximum Transmission Unit) et Fragmentation

### MTU Standard

- **Ethernet standard** : 1500 bytes
- **Overlay networks (VXLAN)** : 1450 bytes (overhead 50 bytes)
- **Jumbo frames** : 9000 bytes (data centers)

### Impact Fragmentation

```
Paquet 2000 bytes avec MTU 1500
    ‚Üì
Fragment 1: 1500 bytes
Fragment 2: 500 bytes
    ‚Üì
Si un fragment perdu ‚Üí TOUT retransmis
```

### Path MTU Discovery

M√©canisme pour d√©terminer le MTU minimum sur le chemin r√©seau :

1. Envoyer paquet avec flag **"Don't Fragment"**
2. Si MTU d√©pass√© ‚Üí routeur retourne **ICMP "Fragmentation Needed"**
3. Source ajuste la taille et r√©essaye

### üîç Tester MTU

```bash
# Test avec ping (1472 + 28 headers = 1500)
ping -M do -s 1472 8.8.8.8

# Si √ßa passe, tester 1473
ping -M do -s 1473 8.8.8.8
# Erreur "Frag needed" ‚Üí MTU < 1501
```

---

## 6. NAT (Network Address Translation)

### Types de NAT

#### SNAT (Source NAT)

Modifie l'IP source sortante. Utilise **PAT** (Port Address Translation).

```
R√©seau priv√©          NAT          Internet
10.0.1.5:54321  ‚Üí  203.0.113.5:12345  ‚Üí  8.8.8.8:53
```

**Limite** : ~65k connexions simultan√©es par IP publique (limitation ports)

#### DNAT (Destination NAT)

Modifie l'IP destination entrante (port forwarding).

```
Internet          NAT          R√©seau priv√©
8.8.8.8  ‚Üí  203.0.113.5:80  ‚Üí  10.0.1.10:8080
```

### Probl√®mes NAT

- **Exhaustion ports SNAT** : Plus de ports disponibles
- **Latence accrue** : Traitement suppl√©mentaire
- **Debugging complexe** : Perte tra√ßabilit√© IP source
- **Incompatibilit√© protocols** : FTP, SIP, IPSec

---

## 7. ARP (Address Resolution Protocol)

### Fonction

R√©sout une **adresse IP** en **adresse MAC** sur un r√©seau local (Layer 2).

### Fonctionnement

```
1. Machine A veut communiquer avec 192.168.1.10
2. A broadcast : "Qui a 192.168.1.10 ?" (ARP Request)
3. Machine avec 192.168.1.10 r√©pond : "C'est moi, MAC: aa:bb:cc:dd:ee:ff"
4. A stocke l'info dans son cache ARP
```

### Commandes

```bash
# Voir le cache ARP
arp -n
ip neigh show

# Vider le cache ARP
ip neigh flush all

# ARP ping
arping -I eth0 192.168.1.10
```

---

## 8. DNS (Domain Name System)

### Hi√©rarchie DNS

```
                    . (root)
                    |
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       com         org         fr
        |           |           |
    google      wikipedia   gouv
        |
      www
```

### Types d'Enregistrements

| Type | Description | Exemple |
|------|-------------|---------|
| **A** | IPv4 | `example.com ‚Üí 93.184.216.34` |
| **AAAA** | IPv6 | `example.com ‚Üí 2606:2800:220:1:...` |
| **CNAME** | Alias | `www.example.com ‚Üí example.com` |
| **MX** | Mail exchange | `example.com ‚Üí mail.example.com` |
| **NS** | Nameserver | `example.com ‚Üí ns1.example.com` |
| **TXT** | Texte arbitraire | SPF, DKIM, v√©rifications |

### Requ√™te DNS

```bash
# R√©solution simple
nslookup google.com

# R√©solution d√©taill√©e
dig google.com

# Sp√©cifier serveur DNS
dig @8.8.8.8 google.com

# Requ√™te inverse (IP ‚Üí nom)
dig -x 93.184.216.34
```

### Cache DNS et TTL

**TTL (Time To Live)** : dur√©e de mise en cache (en secondes)

```
example.com.  300  IN  A  93.184.216.34
              ‚Üë
           TTL = 5 minutes
```

---

## 9. Timeouts, Keep-Alive et Connection Pooling

### Types de Timeouts

| Timeout | Description |
|---------|-------------|
| **Connection timeout** | Dur√©e max pour √©tablir connexion (handshake TCP) |
| **Read timeout** | Dur√©e max pour recevoir donn√©es |
| **Write timeout** | Dur√©e max pour envoyer donn√©es |
| **Idle timeout** | Dur√©e max d'inactivit√© avant fermeture |

### TCP Keep-Alive

M√©canisme pour maintenir connexion active et d√©tecter connexions mortes.

**Param√®tres Linux** :
```bash
# D√©lai avant premier keep-alive (d√©faut: 7200s = 2h)
sysctl net.ipv4.tcp_keepalive_time

# Intervalle entre keep-alives (d√©faut: 75s)
sysctl net.ipv4.tcp_keepalive_intvl

# Nombre de keep-alives avant abandon (d√©faut: 9)
sysctl net.ipv4.tcp_keepalive_probes
```

### Connection Pooling

R√©utilise des connexions TCP √©tablies au lieu d'en cr√©er de nouvelles.

**Avantages** :
- R√©duit latence (pas de handshake)
- R√©duit charge serveur
- √âconomise ressources (ports, file descriptors)

---

# PARTIE 2 - Applications Pratiques (Contexte DevOps/Kubernetes)

## 10. TCP/IP dans Kubernetes : Architecture R√©seau

### 10.1 Mod√®le R√©seau Kubernetes

**Principes fondamentaux** :
1. Chaque **Pod** obtient une IP unique
2. Les Pods communiquent sans NAT entre eux
3. Communication directe peu importe le n≈ìud

### Trois R√©seaux Distincts

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  POD NETWORK (Pod CIDR)                  ‚îÇ
‚îÇ  Exemple: 10.244.0.0/16                  ‚îÇ
‚îÇ  ‚Üí IPs des Pods                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  SERVICE NETWORK (Service CIDR)          ‚îÇ
‚îÇ  Exemple: 10.96.0.0/12                   ‚îÇ
‚îÇ  ‚Üí IPs virtuelles des Services           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  NODE NETWORK                            ‚îÇ
‚îÇ  Exemple: 172.16.0.0/24                  ‚îÇ
‚îÇ  ‚Üí IPs des n≈ìuds Kubernetes              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 10.2 CNI (Container Network Interface)

**R√¥le** : Plugin qui configure le r√©seau pour chaque Pod.

#### Workflow CNI

```
1. Kubelet cr√©e Pod
2. Kubelet appelle CNI plugin
3. CNI cr√©e network namespace
4. CNI alloue IP (via IPAM)
5. CNI configure interfaces (veth pair)
6. CNI configure routes
7. Pod pr√™t avec r√©seau configur√©
```

### 10.3 Overlay vs Underlay Networks

#### üîπ Overlay Networks (Encapsulation)

**Technique** : Encapsulation paquets Pod dans UDP/IP (VXLAN)

```
Original Pod Packet
    ‚Üì
[Outer IP Header][UDP Header][VXLAN Header][Original Packet]
    ‚Üì
Tunnel via UDP port 8472
```

| Avantages | Inconv√©nients |
|-----------|---------------|
| ‚úÖ Isolation L2 | ‚ùå Overhead encapsulation |
| ‚úÖ Pas de config routeurs | ‚ùå MTU r√©duit (1450 vs 1500) |
| ‚úÖ Portable multi-cloud | ‚ùå Performance l√©g√®rement r√©duite |

**Exemples CNI** : Flannel, Weave, Canal (Calico + Flannel)

#### üîπ Underlay Networks (Routage L3)

**Technique** : Routage natif via BGP

```
Pod CIDR routes propag√©es aux routeurs physiques via BGP
Pas d'encapsulation ‚Üí Routage IP natif
```

| Avantages | Inconv√©nients |
|-----------|---------------|
| ‚úÖ Pas d'overhead | ‚ùå Config routeurs n√©cessaire |
| ‚úÖ Performance maximale | ‚ùå Plus complexe |
| ‚úÖ MTU standard (1500) | ‚ùå D√©pendance infrastructure |

**Exemples CNI** : Calico (mode BGP), Cilium

### 10.4 Azure CNI (Contexte AKS)

**Architecture** :
- Chaque Pod = IP du subnet Azure VNet directement
- Pas d'overlay (sauf Azure CNI Overlay mode)
- Routage via Azure SDN

**Avantages** :
- Performance maximale
- Int√©gration native Azure (NSG, UDR, Firewall)
- Pods accessibles depuis VNet

**Inconv√©nient majeur** :
- **Consommation IP √©lev√©e** : 1 Pod = 1 IP subnet
- Risque d'√©puisement IP si mal dimensionn√©

#### Calcul IP Azure CNI

Pour un cluster avec :
- 5 n≈ìuds
- Max 110 Pods par n≈ìud (d√©faut AKS)

**IPs n√©cessaires** :
```
5 n≈ìuds √ó 110 Pods = 550 IPs
+ Marge pour scaling = 800-1000 IPs minimum
‚Üí Subnet /22 (1024 IPs) recommand√©
```

---

## 11. Services Kubernetes et kube-proxy

### Types de Services

| Type | Description | Use Case |
|------|-------------|----------|
| **ClusterIP** | IP interne cluster | Communication interne |
| **NodePort** | Expose sur port n≈ìud (30000-32767) | Dev/test externe |
| **LoadBalancer** | Cr√©e LB cloud (Azure LB, AWS ELB) | Production externe |
| **ExternalName** | Map √† nom DNS externe | Int√©gration services externes |

### kube-proxy : Le Chef d'Orchestre

**R√¥le** : Impl√©mente les r√®gles de forwarding pour les Services

#### Modes kube-proxy

##### 1Ô∏è‚É£ iptables (d√©faut)

```bash
# R√®gles iptables cr√©√©es automatiquement
iptables -t nat -L -n | grep <service-name>

# Exemple de r√®gle
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m tcp --dport 80 \
   -j KUBE-SVC-XXXXX
-A KUBE-SVC-XXXXX -m statistic --mode random --probability 0.5 \
   -j KUBE-SEP-POD1
-A KUBE-SVC-XXXXX -j KUBE-SEP-POD2
```

**Caract√©ristiques** :
- Load balancing al√©atoire
- Scalabilit√© limit√©e (>1000 services)
- Debugging difficile

##### 2Ô∏è‚É£ IPVS (IP Virtual Server)

```bash
# Voir les services IPVS
ipvsadm -Ln

# Exemple output
TCP  10.96.0.10:80 rr
  -> 10.244.1.5:8080      Masq    1      0          0
  -> 10.244.2.8:8080      Masq    1      0          0
```

**Caract√©ristiques** :
- Meilleure performance (grands clusters)
- Algorithmes LB avanc√©s :
  - `rr` : Round Robin
  - `lc` : Least Connection
  - `wrr` : Weighted Round Robin
  - `sh` : Source Hashing

### Flux Connexion Pod ‚Üí Service

```
1. DNS Resolution
   nginx.default.svc.cluster.local ‚Üí ClusterIP (10.96.0.10)

2. Paquet TCP
   SRC: 10.244.1.5:54321
   DST: 10.96.0.10:80

3. kube-proxy (iptables/IPVS)
   DNAT: 10.96.0.10:80 ‚Üí 10.244.2.8:8080 (Pod backend)

4. Routage CNI
   Paquet achemin√© vers Pod cible (m√™me n≈ìud ou autre)

5. R√©ponse
   Pod r√©pond, SNAT inverse appliqu√©
```

---

## 12. Azure Application Gateway et Ingress

### Architecture

```
Internet
   ‚Üì
Azure Public IP
   ‚Üì
Application Gateway (L7)
   ‚Üì
Backend Pool (Pod IPs)
   ‚Üì
AKS Pods
```

### AGIC (Application Gateway Ingress Controller)

Controller Kubernetes qui synchronise les ressources **Ingress** k8s avec la config App Gateway.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: azure/application-gateway
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: example-service
            port:
              number: 80
```

### Flux R√©seau D√©taill√©

```
1. Client ‚Üí App Gateway IP publique
   TCP handshake + TLS termination

2. App Gateway ‚Üí Routing L7
   URL path matching, header inspection

3. App Gateway ‚Üí DNAT vers Pod IP
   Backend pool selection (load balancing)

4. Paquet achemin√© via Azure VNet
   Azure CNI route vers Pod

5. Pod r√©pond
   SNAT (IP source = App Gateway subnet IP)
```

### Points d'Attention TCP/IP

#### MTU
```bash
# V√©rifier MTU coh√©rent
ip link show | grep mtu

# Tester fragmentation
ping -M do -s 1472 <pod-ip>
```

#### SNAT Exhaustion

**Limite** : ~64k connexions simultan√©es par IP backend pool

**Solutions** :
- Augmenter nombre d'instances App Gateway
- Configurer **connection draining**
- Activer **HTTP keep-alive**

```yaml
# Dans le Pod
server {
    keepalive_timeout 65;
    keepalive_requests 100;
}
```

#### Health Probes

App Gateway fait des health checks vers Pods.

**Configuration** :
- Protocol : HTTP ou HTTPS
- Path : `/health` ou `/ready`
- Interval : 30s (d√©faut)
- Timeout : 30s (d√©faut)
- Unhealthy threshold : 3 (d√©faut)

```bash
# V√©rifier health depuis App Gateway subnet
curl -v http://<pod-ip>:8080/health
```

---

## 13. Network Policies : Firewall L3/L4 dans Kubernetes

### Principe

**Deny-by-default** d√®s qu'une policy s√©lectionne un Pod.

### Exemple : Isoler un namespace

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: production
spec:
  podSelector: {}  # S√©lectionne tous les pods
  policyTypes:
  - Ingress
  - Egress
  # Pas de r√®gles = tout bloqu√©
```

### Exemple : Autoriser trafic sp√©cifique

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

### Exemple : Autoriser egress vers Internet

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-api
spec:
  podSelector:
    matchLabels:
      app: api-client
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32  # Bloquer metadata service
    ports:
    - protocol: TCP
      port: 443
```

### Impl√©mentation CNI

**Azure CNI standard** : Ne supporte PAS Network Policies

**Solutions** :
- **Azure Network Policy Manager**
- **Calico** (installer en addon)
- **Cilium**

```bash
# Installer Calico sur AKS
az aks create \
  --network-plugin azure \
  --network-policy calico
```

---

## 14. Troubleshooting R√©seau : Outils et M√©thodologie

### 14.1 Outils Essentiels

#### üîß tcpdump

Capture paquets r√©seau au niveau interface.

```bash
# Capturer tout le trafic sur eth0
tcpdump -i eth0 -n

# Filtrer par port
tcpdump -i eth0 port 80

# Filtrer par host
tcpdump -i eth0 host 10.244.1.5

# Voir flags TCP
tcpdump -i eth0 'tcp[tcpflags] & (tcp-syn) != 0'

# Sauvegarder pour analyse Wireshark
tcpdump -i eth0 -w capture.pcap
```

**Filtres utiles** :
```bash
# Voir uniquement SYN packets
tcpdump 'tcp[tcpflags] & tcp-syn != 0'

# Voir RST (connexion refus√©e)
tcpdump 'tcp[tcpflags] & tcp-rst != 0'

# Exclure SSH (pour pas polluer)
tcpdump 'not port 22'
```

#### ss (socket statistics)

```bash
# Toutes connexions TCP/UDP
ss -tunap

# Connexions √©tablies
ss -tun state established

# Connexions en TIME_WAIT
ss -tun state time-wait

# Statistiques globales
ss -s

# √âcoute sur quel port
ss -tlnp | grep :8080
```

#### netstat (legacy)

```bash
# Connexions actives
netstat -tunap

# Table de routage
netstat -rn

# Statistiques interfaces
netstat -i
```

#### ping / traceroute

```bash
# Test connectivit√© ICMP
ping -c 4 8.8.8.8

# Traceroute
traceroute google.com
# ou
tracepath google.com

# Traceroute TCP (contourne blocage ICMP)
tcptraceroute google.com 443
```

#### curl / wget

```bash
# Test HTTP avec d√©tails
curl -v https://example.com

# Voir timing d√©taill√©
curl -w "@curl-format.txt" -o /dev/null -s https://example.com

# curl-format.txt :
time_namelookup:  %{time_namelookup}s
time_connect:     %{time_connect}s
time_appconnect:  %{time_appconnect}s
time_pretransfer: %{time_pretransfer}s
time_starttransfer: %{time_starttransfer}s
time_total:       %{time_total}s
```

#### nslookup / dig

```bash
# R√©solution DNS simple
nslookup google.com

# R√©solution d√©taill√©e
dig google.com

# Sp√©cifier serveur DNS
dig @8.8.8.8 google.com

# Reverse DNS
dig -x 8.8.8.8

# Voir TTL
dig google.com +noall +answer
```

#### iptables / nft

```bash
# Lister toutes les r√®gles
iptables -L -n -v

# Table NAT (SNAT/DNAT)
iptables -t nat -L -n -v

# Compter paquets par r√®gle
iptables -L -n -v -x

# Tracer un paquet (debug)
iptables -t raw -A PREROUTING -p tcp --dport 80 -j TRACE
```

### 14.2 M√©thodologie de Debugging

#### √âtape 1 : Identifier la Couche

```
L7 (Application) ?
‚Üí Erreurs HTTP 4xx/5xx, timeouts applicatifs

L4 (Transport) ?
‚Üí Connection refused, timeout TCP, handshake fail

L3 (IP) ?
‚Üí Routage incorrect, paquet perdu, MTU issues

L2 (Liaison) ?
‚Üí Probl√®me ARP, switch, VLAN
```

#### √âtape 2 : V√©rifier Connectivit√© de Base

```bash
# Test ICMP
ping <destination-ip>

# Test port TCP
telnet <ip> <port>
# ou
nc -zv <ip> <port>

# Test DNS
nslookup <hostname>
dig <hostname>
```

#### √âtape 3 : Capturer et Analyser Trafic

```bash
# Sur source ET destination
tcpdump -i any -n host <ip>

# Analyser flags TCP
# SYN envoy√© mais pas de SYN-ACK ?
#   ‚Üí Firewall ou service down
# RST re√ßu ?
#   ‚Üí Port ferm√©
# Retransmissions (duplicate ACK) ?
#   ‚Üí Probl√®me r√©seau (latence, perte)
```

#### √âtape 4 : V√©rifier Routage

```bash
# Quelle route utilis√©e ?
ip route get <destination-ip>

# Table de routage compl√®te
ip route show

# V√©rifier si paquet transite par bonne interface
tcpdump -i eth0 host <destination-ip>
```

#### √âtape 5 : Analyser √âtats Connexions

```bash
# Stats globales
ss -s

# Beaucoup de TIME_WAIT ?
ss state time-wait | wc -l
# ‚Üí Check tcp_tw_reuse, timeouts TCP

# CLOSE_WAIT √©lev√© ?
ss state close-wait | wc -l
# ‚Üí App ne ferme pas connexions
```

### 14.3 Debug Kubernetes Sp√©cifique

#### Test Pod ‚Üí Pod

```bash
# Depuis un Pod source
kubectl exec -it pod-source -- ping <pod-destination-ip>
kubectl exec -it pod-source -- curl http://<pod-destination-ip>:8080

# V√©rifier CNI
kubectl logs -n kube-system -l k8s-app=calico-node
```

#### Test Service DNS

```bash
# R√©solution interne
kubectl exec -it pod -- nslookup service.namespace.svc.cluster.local

# V√©rifier CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl logs -n kube-system -l k8s-app=kube-dns

# Config CoreDNS
kubectl get configmap -n kube-system coredns -o yaml
```

#### Test Connectivit√© Externe

```bash
# Depuis Pod
kubectl exec -it pod -- curl -v https://google.com

# V√©rifier egress (NSG, Firewall, Network Policy)
kubectl get networkpolicies -A
```

#### Debug Service

```bash
# Voir endpoints du Service
kubectl get endpoints <service-name>

# Si endpoints vide ‚Üí Probl√®me selector ou Pods pas ready
kubectl get pods -l <selector>
kubectl describe pod <pod-name>

# V√©rifier kube-proxy
kubectl logs -n kube-system -l k8s-app=kube-proxy
```

---

## 15. Cas Pratiques : Contexte Promod/AKS

### 15.1 √âpuisement du Service CIDR

#### Sympt√¥me

```
Error creating service: no IPs available in range
```

#### üîç Diagnostic

```bash
# Compter les Services existants
kubectl get services -A | wc -l

# V√©rifier Service CIDR configur√©
kubectl cluster-info dump | grep service-cluster-ip-range

# Calculer IPs disponibles
# Ex: /20 = 2^12 = 4096 IPs
# Mais attention aux Services headless qui consomment aussi !
```

#### Solution

**Court terme** :
```bash
# Nettoyer Services inutilis√©s
kubectl get svc -A | grep <pattern>
kubectl delete svc <service-name> -n <namespace>
```

**Long terme** :
```bash
# Recr√©er cluster avec Service CIDR plus large
az aks create \
  --service-cidr 10.96.0.0/12 \  # Au lieu de /20
  --dns-service-ip 10.96.0.10
```

#### üõ°Ô∏è Pr√©vention

**Calcul dimensionnement** :
```
Services max pr√©vus : 500
Services headless : 100 (multiplient par nombre de Pods)
Marge s√©curit√© : 2x

‚Üí Service CIDR : /16 minimum (65k IPs)
   Id√©alement : /12 (1M IPs)
```

### 15.2 Probl√®mes Application Gateway

#### Sympt√¥mes

- Timeouts HTTP al√©atoires
- 502 Bad Gateway
- Latence √©lev√©e

#### üîç Causes Possibles

##### 1. Exhaustion SNAT

```bash
# V√©rifier m√©triques Azure
az monitor metrics list \
  --resource <app-gateway-id> \
  --metric "SNAT Port Utilization"

# Si >80% ‚Üí Probl√®me SNAT
```

**Solution** :
- Augmenter instances App Gateway
- Activer connection pooling c√¥t√© backend
- Configurer keep-alive

```nginx
# Dans le Pod nginx
http {
    upstream backend {
        server backend:8080;
        keepalive 32;  # Connection pool
    }
}
```

##### 2. Health Probes Fails

```bash
# Tester health check depuis App Gateway subnet
curl -v http://<pod-ip>:8080/health

# V√©rifier logs backend
kubectl logs <pod-name> | grep health

# Ajuster probe settings
```

```yaml
# Dans l'Ingress
metadata:
  annotations:
    appgw.ingress.kubernetes.io/health-probe-interval: "30"
    appgw.ingress.kubernetes.io/health-probe-timeout: "30"
    appgw.ingress.kubernetes.io/health-probe-unhealthy-threshold: "3"
```

##### 3. NSG/Firewall Blocking

```bash
# V√©rifier NSG rules
az network nsg rule list \
  --resource-group <rg> \
  --nsg-name <nsg-name> \
  --output table

# R√®gles n√©cessaires :
# App Gateway subnet ‚Üí AKS subnet : ports backend (80, 443, custom)
# App Gateway subnet ‚Üí Internet : port 443 (health checks externe)
```

##### 4. MTU Mismatch

```bash
# Tester MTU
ping -M do -s 1472 <pod-ip>  # 1472 + 28 = 1500

# Si erreur "Frag needed" ‚Üí Ajuster MTU
ip link set dev eth0 mtu 1450
```

### 15.3 Migration Workload Identity

#### Contexte

**Avant** : Azure AD Pod Identity (inject proxy sidecar)  
**Apr√®s** : Workload Identity (OIDC native)

#### Impact R√©seau

**Pod Identity** :
```
Pod ‚Üí Proxy sidecar (NMI) ‚Üí IMDS (169.254.169.254) ‚Üí Azure AD
        ‚Üë Overhead r√©seau
```

**Workload Identity** :
```
Pod ‚Üí Service Account Token ‚Üí Azure AD (direct OIDC)
        ‚Üë Pas de proxy
```

#### Points d'Attention TCP/IP

```bash
# V√©rifier connectivit√© Azure AD endpoints
kubectl exec -it pod -- curl -v https://login.microsoftonline.com

# V√©rifier egress autoris√© (NSG/Firewall)
# Azure AD endpoints :
# - login.microsoftonline.com (443)
# - *.graph.microsoft.com (443)
```

### 15.4 Debug Production : Checklist

#### Pod ne d√©marre pas

```bash
# 1. V√©rifier events
kubectl describe pod <pod-name>

# 2. V√©rifier image pull
kubectl get events --field-selector involvedObject.name=<pod-name>

# 3. V√©rifier r√©seau
kubectl exec -it <pod-name> -- ip addr
kubectl exec -it <pod-name> -- ip route
```

#### Service inaccessible

```bash
# 1. V√©rifier endpoints
kubectl get endpoints <service-name>

# 2. Si vide, v√©rifier selector
kubectl get pods -l <selector> -o wide

# 3. Tester depuis un autre Pod
kubectl run -it --rm debug --image=busybox --restart=Never -- sh
  wget -O- http://<service-name>.<namespace>.svc.cluster.local

# 4. V√©rifier Network Policies
kubectl get netpol -n <namespace>
```

#### Latence √©lev√©e

```bash
# 1. Mesurer latence r√©seau
kubectl exec -it pod-source -- ping <pod-destination-ip>

# 2. V√©rifier retransmissions TCP
kubectl exec -it pod-source -- netstat -s | grep retrans

# 3. Analyser avec tcpdump
kubectl exec -it pod -- tcpdump -i eth0 -w /tmp/capture.pcap
kubectl cp pod:/tmp/capture.pcap ./capture.pcap
# Analyser avec Wireshark

# 4. V√©rifier CPU/Memory du Pod
kubectl top pod <pod-name>
```

---

## 16. Best Practices TCP/IP pour Infrastructure Kubernetes

### Dimensionnement R√©seau

#### Pod CIDR

```
Calcul :
- N≈ìuds max : 50
- Pods max par n≈ìud : 110 (d√©faut AKS)
- IPs n√©cessaires : 50 √ó 110 = 5,500

‚Üí Pod CIDR : /16 (65k IPs) 
   Alternative : /17 (32k IPs) si budget IP limit√©
```

#### Service CIDR

```
Calcul :
- Services max : 1,000
- Marge s√©curit√© : 3x
- IPs n√©cessaires : 3,000

‚Üí Service CIDR : /16 (65k IPs)
   Id√©alement : /12 (1M IPs) pour √©viter tout risque
```

#### Node Subnet (Azure CNI)

```
Calcul :
- N≈ìuds actuels : 20
- N≈ìuds max (avec scaling) : 50
- Marge : 2x

‚Üí Node subnet : /25 (128 IPs) minimum
   Recommand√© : /24 (256 IPs)
```

### MTU Optimization

```bash
# Overlay networks (VXLAN)
ip link set dev eth0 mtu 1450

# Underlay / Azure CNI
ip link set dev eth0 mtu 1500

# Jumbo frames (on-prem datacenter)
ip link set dev eth0 mtu 9000
```

### Connection Pooling

#### Application Level

```python
# Python example
import requests
from requests.adapters import HTTPAdapter

session = requests.Session()
adapter = HTTPAdapter(
    pool_connections=100,
    pool_maxsize=100,
    max_retries=3
)
session.mount('http://', adapter)
session.mount('https://', adapter)
```

```go
// Go example
client := &http.Client{
    Transport: &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 100,
        IdleConnTimeout:     90 * time.Second,
    },
}
```

#### Database Connections

```yaml
# PostgreSQL example
env:
- name: DB_POOL_SIZE
  value: "20"
- name: DB_POOL_TIMEOUT
  value: "30"
- name: DB_MAX_OVERFLOW
  value: "10"
```

### Kernel Tuning (Linux)

```bash
# Augmenter limites connexions
sysctl -w net.core.somaxconn=4096
sysctl -w net.ipv4.tcp_max_syn_backlog=4096

# R√©utiliser ports TIME_WAIT plus rapidement
sysctl -w net.ipv4.tcp_tw_reuse=1

# Ajuster timeouts TCP
sysctl -w net.ipv4.tcp_keepalive_time=600
sysctl -w net.ipv4.tcp_keepalive_intvl=60
sysctl -w net.ipv4.tcp_keepalive_probes=3

# Augmenter buffer sizes
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
```

### Monitoring R√©seau

#### M√©triques Cl√©s

| M√©trique | Seuil Alert | Action |
|----------|-------------|--------|
| **Latency (RTT)** | >100ms | Investiguer routage/congestion |
| **Packet loss** | >1% | Check infrastructure r√©seau |
| **TCP retransmissions** | >1% | Analyser qualit√© r√©seau |
| **Connexions TIME_WAIT** | >10k | Ajuster timeouts, activer tw_reuse |
| **SNAT port usage** | >80% | Scale out, optimize connection pooling |

#### Prometheus Queries

```promql
# Latency moyenne
rate(http_request_duration_seconds_sum[5m])
/ rate(http_request_duration_seconds_count[5m])

# Taux d'erreur r√©seau
rate(network_errors_total[5m])

# Connexions actives
node_netstat_Tcp_CurrEstab

# TIME_WAIT connexions
node_sockstat_TCP_tw
```

### S√©curit√© R√©seau

#### Network Policies : Strat√©gie Deny-All

```yaml
# 1. Deny everything par d√©faut
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 2. Autoriser uniquement le n√©cessaire
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-backend
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080
```

#### TLS/mTLS

```yaml
# Avec Istio
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: production
spec:
  mtls:
    mode: STRICT  # Force mTLS
```

#### Egress Control

```yaml
# Limiter egress Internet
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-apis
spec:
  podSelector:
    matchLabels:
      app: api-client
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8      # RFC 1918
        - 172.16.0.0/12   # RFC 1918
        - 192.168.0.0/16  # RFC 1918
        - 169.254.169.254/32  # Metadata service
    ports:
    - protocol: TCP
      port: 443
```

---

## Ressources Recommand√©es

### RFCs Officielles

| RFC | Sujet | Lien |
|-----|-------|------|
| **RFC 791** | Internet Protocol (IP) | [rfc-editor.org/rfc/rfc791](https://www.rfc-editor.org/rfc/rfc791) |
| **RFC 793** | Transmission Control Protocol (TCP) | [rfc-editor.org/rfc/rfc793](https://www.rfc-editor.org/rfc/rfc793) |
| **RFC 768** | User Datagram Protocol (UDP) | [rfc-editor.org/rfc/rfc768](https://www.rfc-editor.org/rfc/rfc768) |
| **RFC 1180** | TCP/IP Tutorial | [rfc-editor.org/rfc/rfc1180](https://www.rfc-editor.org/rfc/rfc1180) |
| **RFC 1918** | Private Address Space | [rfc-editor.org/rfc/rfc1918](https://www.rfc-editor.org/rfc/rfc1918) |
| **RFC 826** | Address Resolution Protocol (ARP) | [rfc-editor.org/rfc/rfc826](https://www.rfc-editor.org/rfc/rfc826) |

### Livres

- **TCP/IP Illustrated, Volume 1** - W. Richard Stevens  
  *La r√©f√©rence absolue sur TCP/IP*

- **IBM Redbook: TCP/IP Tutorial and Technical Overview**  
  *Gratuit, ~900 pages, tr√®s complet*  
  [ibm.com/redbooks](https://www.redbooks.ibm.com/abstracts/gg243376.html)

### üéì Kubernetes Networking

- **CNI Specification**  
  [github.com/containernetworking/cni](https://github.com/containernetworking/cni)

- **The Kubernetes Networking Guide**  
  [tkng.io](https://www.tkng.io)

- **Azure CNI Documentation**  
  [docs.microsoft.com/azure/aks/configure-azure-cni](https://docs.microsoft.com/azure/aks/configure-azure-cni)

### Outils Pratiques

| Outil | Description | Installation |
|-------|-------------|--------------|
| **Wireshark** | Analyse graphique captures r√©seau | [wireshark.org](https://www.wireshark.org) |
| **mtr** | Traceroute + ping combin√©s | `apt install mtr` |
| **netcat (nc)** | Swiss army knife r√©seau | `apt install netcat` |
| **kubectl-debug** | Debug pods r√©seau dans k8s | [github.com/aylei/kubectl-debug](https://github.com/aylei/kubectl-debug) |
| **termshark** | Wireshark en CLI | `apt install termshark` |
| **tcptrace** | Analyse avanc√©e captures TCP | `apt install tcptrace` |

### Formations

- **Azure AZ-104** (certification)  
  Section networking tr√®s compl√®te

- **Kubernetes CKA/CKAD**  
  Networking troubleshooting

- **Linux Foundation: Kubernetes Networking**  
  [training.linuxfoundation.org](https://training.linuxfoundation.org)

---

## Conclusion

La ma√Ætrise de TCP/IP est **fondamentale** pour un ing√©nieur infrastructure moderne, particuli√®rement dans un contexte cloud et Kubernetes.

### Points Cl√©s √† Retenir

1. **Comprendre les couches** : OSI/TCP-IP aide √† identifier o√π se situe un probl√®me
2. **Ma√Ætriser le subnetting** : Crucial pour dimensionner correctement (Pod/Service/Node CIDRs)
3. **Conna√Ætre TCP vs UDP** : Choisir le bon protocole selon les besoins (fiabilit√© vs latence)
4. **D√©bugger m√©thodiquement** : tcpdump, ss, et analyse des flags TCP sont tes meilleurs amis
5. **Anticiper les limites** : SNAT exhaustion, MTU, Service CIDR
